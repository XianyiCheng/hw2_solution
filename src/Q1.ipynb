{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.activate(joinpath(@__DIR__,\"..\")); Pkg.instantiate()\n",
    "using Test\n",
    "using Plots\n",
    "include(\"quadruped.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Balancing the Quadruped with LQR (25 pts)\n",
    "In this problem you'll stabilize the quadruped around the equilibrium point you found in the previous homework.\n",
    "\n",
    "Note that we modified the model a little from the previous HW to make this problem easier. It now has once less degree of freedom (DOF), since we aren't allowing it to twist on the floor. Assuming we have \"grippy\" rubber feet, this isn't a bad assumption to make. This means it now has 28 states (14 positions, 14 velocities) and 12 controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the quadruped model\n",
    "model = UnitreeA1()\n",
    "n,m = state_dim(model), control_dim(model)\n",
    "\n",
    "# Use the equilibrium point we found in HW1\n",
    "xeq = [0.30876598892362367, -0.260133570638461, -0.09358998727238592, 0.7078311865326717, -0.41038927424745475, -0.3598290230260733, 0.35837932714316556, 0.3579443729935737, -0.52325974352618, -0.524349602250509, 0.5321637004468419, 0.17483502352564967, 0.1745288580720117, -0.17241139464042135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "ueq = [7.326746743908579, 12.492831761746691, -3.7276131797642345, -0.21210770886642585, 0.7228048164769122, 0.7061868479839222, -0.954088911642226, -0.8136046730618279, -0.5649940436402389, -0.273344715530589, -0.2382444936066465, -0.19338784324481662]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a): Compute the optimal LQR gains using Riccati (8 pts)\n",
    "As we saw in class, we can solve the LQR problem using a backward Riccati recursion, which also generates a locally-optimal feedback controller. Implement your own method to calculate a set of optimal feedback gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Implement the following function (5 pts)\n",
    "\"\"\"\n",
    "    riccati(A,B,Q,R,Qf,N)\n",
    "\n",
    "Use backward riccati recursion to solve the finite-horizon time-invariant LQR problem.\n",
    "Returns vectors of the feedback gains `K` and cost-to-go matrices `P`, where `length(K) == N-1`,\n",
    "`length(P) == N`, and `size(K[i]) == (m,n)` and `size(P[i]) == (n,n)`.\n",
    "\n",
    "# Arguments:\n",
    "* `A`: `(n,n)` discrete dynamics Jacobian wrt the state\n",
    "* `B`: `(n,m)` discrete dynamics Jacobian wrt the control\n",
    "* `Q`: `(n,n)` stage-wise cost matrix for states\n",
    "* `R`: `(m,m)` stage-wise cost matrix for controls\n",
    "* `Qf`: `(n,n)` cost matrix for terminal state\n",
    "* `N`: Integer number of time steps (horizon length).\n",
    "\"\"\"\n",
    "function riccati(A,B,Q,R,Qf,N)\n",
    "    # initialize the output\n",
    "    n,m = size(B)\n",
    "    P = [zeros(n,n) for k = 1:N]\n",
    "    K = [zeros(m,n) for k = 1:N-1]\n",
    "    \n",
    "    # TODO: implement the Riccati recursion\n",
    "    \n",
    "    # return the feedback gains and ctg matrices\n",
    "    return K,P\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Compute the optimal gains (3 pts)\n",
    "#       Store the result in a matrix K::Vector{Matrix{Float64}}, where size(K[1]) = (12,28)\n",
    "#       Use the cost matrices provided below\n",
    "\n",
    "# Some parameters\n",
    "dt = 0.01\n",
    "tf = 2.0\n",
    "times = range(0,tf,step=dt)\n",
    "N = length(times)\n",
    "\n",
    "# Define the LQR cost\n",
    "Q = Diagonal([fill(130.,nÃ·2); fill(130.,nÃ·2)])\n",
    "R = Diagonal(fill(10.0, m))\n",
    "Qf = copy(Q)\n",
    "\n",
    "# TODO: solve for the feedback gains\n",
    "A = zeros(n,n)\n",
    "B = zeros(n,m)\n",
    "K,P = riccati(A,B,Q,R,Qf,N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Implement the LQR Controller (3 pts)\n",
    "Now that we've computed our gains, let's implement a controller to use on our system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: implement the following methods\n",
    "#       get_control\n",
    "\"\"\"\n",
    "    LQRController\n",
    "\n",
    "Type for evaluting a time-invariant LQR control policy. If the `infinite_horizon`\n",
    "field is true, it will only use a single gain, `K[1]`.\n",
    "\"\"\"\n",
    "struct LQRController\n",
    "    K::Vector{Matrix{Float64}}   # feedback gains ((m,n),N-1)\n",
    "    times::Vector{Float64}       # times          (N,)\n",
    "    xeq::Vector{Float64}         # equilibrium states\n",
    "    ueq::Vector{Float64}         # equilibrium controls\n",
    "    infinite_horizon::Bool       # use infinite horizon control\n",
    "end\n",
    "function LQRController(K,xeq,ueq,tf, ih=false)\n",
    "    LQRController(Matrix.(K), collect(range(0,tf,length=length(K)+1)), xeq, ueq, ih)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    get_k(ctrl, t)\n",
    "\n",
    "Get the time index corresponding to time `t`. \n",
    "Useful for implementing zero-order hold control.\n",
    "Uses binary search to find the time index.\n",
    "\"\"\"\n",
    "get_k(controller::LQRController, t) = searchsortedlast(controller.times, t)\n",
    "\n",
    "\"\"\"\n",
    "    get_control(ctrl::LQRController, x, t)\n",
    "\n",
    "Evaluate the LQRController feedback policy at state `x` and time `t`, returning the control \n",
    "to be executed by the system.\n",
    "\"\"\"\n",
    "function get_control(controller::LQRController, x, t)\n",
    "    # TODO: Finish the function to calculate the control at time t\n",
    "    return zero(controller.ueq)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the controller\n",
    "ctrl = LQRController(K,xeq,ueq,tf)\n",
    "utest = get_control(ctrl, xeq, 0.0) - ueq\n",
    "@test norm(utest) â‰ˆ 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c): Compute the infinite-horizon gain (6 pts)\n",
    "In the previous question, we found a set of time-varying gains to stabilize our system. However, let's look at the behavior of our gains over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "Kmat = hcat(vec.(K)...);\n",
    "plot(times[1:end-1], Kmat', legend=:none, xlabel=\"time (s)\", ylabel=\"feedback gains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like they're starting to converge to some steady-state initial gain. Let's increase the length of our horizon and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extend the horizon by at least 2x and save the new gains and cost-to-go in Kinf, Pinf (3 pts)\n",
    "Kinf,Pinf = deepcopy(K), deepcopy(P)\n",
    "\n",
    "# Plot the result\n",
    "Kmat = hcat(vec.(Kinf)...);\n",
    "plot(Kmat', legend=:none, xlabel=\"time (s)\", ylabel=\"feedback gains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see that they're definitely converging to some steady-state initial value. This value is usually referred to as the \"infinite horizon\" gain, and is very useful in practice. It's common practice to use the associated cost-to-go matrix as the $Q_f$ weighting matrix in finite-horizon problems (we'll do this in Q3).\n",
    "\n",
    "**TASK**: Tweak your controller code above so that if the `infinite_horizon` flag is true, it only uses the gains at the first time step. Then generate a new controller, `ctrl_inf` that only uses the infinite gain.\n",
    "\n",
    "With both the finite and infinite-horizon controllers defined, let's see how they do on the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create the infinite-horizon controller (3 pts)\n",
    "ctrl_inf = ctrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d): Stability Analysis (3 pts)\n",
    "Before we simulate, it's always a good idea to check the stability of our system. Evaluate the stability using the infinite-horizon gain and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: compute the stability of our original uncontrolled system\n",
    "#       i.e. norm of largest eigenvalue\n",
    "stability0 = NaN\n",
    "\n",
    "# TASK: compute the stability of our new, controlled system\n",
    "stability = NaN\n",
    "\n",
    "@test stability0 > 1\n",
    "@test stability < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (e): Simulate the Quadruped (5 pts)\n",
    "Let's now put it all together and stabilize the quadruped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: complete the simulator code (5 pts)\n",
    "\"\"\"\n",
    "    simulate(model, x0, controller; dt, tf, mvis)\n",
    "\n",
    "Simulate the quadruped, starting from initial state `x0` and using `controller` to stabilize the system.\n",
    "\n",
    "# Keyword Arguments\n",
    "* `tf`: total simulation time\n",
    "* `dt`: simulation time step\n",
    "* `mvis`: if passed in, use the `MechanismVisualizer` to visualize the simulation while it's running\n",
    "\"\"\"\n",
    "function simulate(model::UnitreeA1, x0, controller; dt=0.05, tf=1.0, mvis=nothing)\n",
    "    # some initialization\n",
    "    time = range(0, tf, step=dt)\n",
    "    n,m = state_dim(model), control_dim(model) \n",
    "    N = Int(round(tf/dt)) + 1\n",
    "    X = [@SVector zeros(n) for k = 1:N] \n",
    "    U = [@SVector zeros(m) for k = 1:N-1] \n",
    "    \n",
    "    # set the initial state\n",
    "    X[1] = x0\n",
    "\n",
    "    for k = 1:length(time) - 1\n",
    "        # TODO: simulate the system with feedback\n",
    "\n",
    "        # visualization code\n",
    "        if !isnothing(mvis)\n",
    "            set_configuration!(mvis, X[k+1][1:14])\n",
    "            sleep(dt)\n",
    "        end\n",
    "    end\n",
    "    return X,U,time\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvis = initialize_visualizer(model)\n",
    "render(mvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturb the initial condition\n",
    "x_init = copy(xeq)\n",
    "x_init[18] -= 0.1\n",
    "x_init[19] -= 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite Horizon\n",
    "X,U,time = simulate(model, x_init, ctrl, dt=dt, tf=tf);\n",
    "visualize!(mvis, model, time[end], X)  # send the trajectory to the visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infinite horizon\n",
    "Xinf,Uinf,time = simulate(model, x_init, ctrl_inf, dt=dt, tf=tf);\n",
    "visualize!(mvis, model, time[end], X)  # send the trajectory to the visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look some plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the response\n",
    "c = [:red :green :blue]\n",
    "dX = [x - xeq for x in X]\n",
    "plot(times,dX,inds=3:5, legend=:none, xlabel=\"time (s)\", ylabel=\"joint positions (rad)\", c=c)\n",
    "dX2 = [x - xeq for x in Xinf]\n",
    "plot!(times, dX2, inds=3:5, legend=:none, colors=1:12, ls=:dash, c=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dU = [u - ueq for u in U]\n",
    "plot(times[1:end-1], dU, inds=1:3, xlabel=\"time (s)\", ylabel=\"joint torques\", c=c)\n",
    "dU2 = [u - ueq for u in Uinf]\n",
    "plot!(times[1:end-1], dU2, inds=1:3, ls=:dash, c=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
